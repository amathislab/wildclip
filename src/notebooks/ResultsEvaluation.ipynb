{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7539dd",
   "metadata": {},
   "source": [
    " WildCLIP by Gabeff et al.  \n",
    "Â© ECEO and A. Mathis Lab  \n",
    "https://github.com/amathislab/wildclip   \n",
    "\n",
    "Licensed under GNU Lesser General Public License v3.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdb64e6-bd14-4303-bad8-81da0f081f90",
   "metadata": {},
   "source": [
    "# Performance evaluation\n",
    "\n",
    "Given a prediction file, the original input file, and a list of queries, computes the mAP for these test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1defa6-fd9d-4c84-b6dc-df26f8902221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from utils import visualization as viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29083c26-5972-4849-8b01-3251ab502fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to cropped images\n",
    "data_path = Path(\"../<path_to_test_images>/\")\n",
    "\n",
    "# Path to output csv from eval_clip.py\n",
    "predictions_file = Path(\"../<path_to_output_predictions>\")\n",
    "\n",
    "# Path to input csv to eval_clip.py\n",
    "annotations_file = Path(\"../<path_to_input_csv_to_test>\")\n",
    "\n",
    "# Path to queries to compute performance for. \n",
    "# Must have an entry both in the \"true\" column of the prediction file (e.g. has ground truth) \n",
    "# and be one of the test column (e.g. \"similarity has been computed\").\n",
    "# Must contain entries: \n",
    "#   \"query\": the query to test\n",
    "#   \"template\": id of the template, either 1, 8, 9 or 10\n",
    "#   \"n_attributes\": number of attributes in the query\n",
    "# \n",
    "# Alternatively, can be a text file with a test query per row\n",
    "queries_path = Path(\"../captions/<path_to_test_queries>\") \n",
    "\n",
    "\n",
    "# We test with queries following either template 1 or template 8-10\n",
    "templates = [\"template_1\"]\n",
    "#templates = [\"template_8\", \"template_9\", \"template_10\"]\n",
    "\n",
    "# Number of attributes in a test query: e.g \"A camera trap picture of a lion eating\" contains two attributes (\"lion\" and \"eating\" for species and behavior, respectively). \n",
    "# For simplicitiy in the paper, we tested with n_attr = 1 only.\n",
    "n_attr = 1\n",
    "\n",
    "# We test with queries containing either words from the base or the novel vocabulary\n",
    "with open(\"../captions/ood_words.txt\", \"r\") as f:\n",
    "    ood_words = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a9d5d-5f3d-4f3c-87d1-1fb168a97f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision_score(queries, results_df):\n",
    "    \"\"\"Computes mAP for a set of query and predicted relevance score to each query\"\"\"\n",
    "    APi = {}\n",
    "    for query in queries:\n",
    "        true = results_df[\"true\"].apply(lambda d: query in d).astype(int).values\n",
    "        if np.sum(true) == 0:\n",
    "            #Query never seen in ground truth.\n",
    "            APi[query] = np.nan\n",
    "            continue\n",
    "        scores = results_df[query].values #Cosine similarity between this query and all images\n",
    "        AveP = average_precision_score(true, scores)\n",
    "        APi[query] = AveP\n",
    "    \n",
    "    if len(queries): \n",
    "        return np.nanmean(list(APi.values())), APi\n",
    "    else:\n",
    "        return np.nan, APi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62882d31-2625-413b-92ba-82fd4e53fe09",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## One result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a66ea02-7d56-464a-83bc-119e7f1b9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.read_csv(predictions_file, index_col=0)\n",
    "annotations_df = pd.read_csv(annotations_file, index_col=0)\n",
    "results_df = (\n",
    "    annotations_df[[\"crop_path\", \"CaptureEventID\"]]\n",
    "    .merge(predictions_df, left_index=True, right_index=True) #test and input csv crops have the same index\n",
    ")\n",
    "\n",
    "# Each image corresponds to many captions, all separated by a \"; \"\n",
    "results_df[\"true\"] = results_df[\"true\"].str.split(\"; \")\n",
    "\n",
    "# We get the queries that matches parameters of interest, that have at least one ground truth image, and the cosine similarity with test images have been computed\n",
    "if queries_path.suffix == \".txt\":\n",
    "    with open(queries_path, \"r\") as f:\n",
    "        queries = f.read().splitlines()\n",
    "elif queries_path.suffix == \".csv\":\n",
    "    queries_df = pd.read_csv(queries_path, index_col=0)\n",
    "    queries_subset_df = queries_df[(queries_df[\"template\"].isin(templates)) & (queries_df[\"n_attributes\"] == n_attr)]\n",
    "    queries = queries_subset_df[\"query\"].values\n",
    "queries = list(set(queries).intersection(set([q for qs in results_df[\"true\"].values for q in qs])))\n",
    "queries = list(set(queries).intersection(set(results_df.columns)))\n",
    "new_queries = [q for q in queries if any([o in q for o in ood_words])]\n",
    "base_queries = list(set(queries) - set(new_queries))\n",
    "\n",
    "# We take max similarity for all queries along the event images to also get predictions at the event level\n",
    "predictions_event_df = (\n",
    "    results_df\n",
    "    [queries+[\"CaptureEventID\"]]\n",
    "    .groupby(\"CaptureEventID\")\n",
    "    .max() # for each column, which are the similarity between test queries and test images\n",
    "    .reset_index()\n",
    ")\n",
    "results_event_df = results_df.drop(queries, axis=1).merge(predictions_event_df, on=\"CaptureEventID\").drop_duplicates(\"CaptureEventID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c928b31d-24fe-4a7c-a655-8de5fddd94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute performance separately for test base and test novel queries, at the image and event level \n",
    "base_mAP, base_APi = mean_average_precision_score(base_queries, results_df)\n",
    "new_mAP, new_APi = mean_average_precision_score(new_queries, results_df)\n",
    "print(\"*\", predictions_file.stem, f\"mAP base classes at crop level: {base_mAP: .3f}\")\n",
    "print(\"*\", predictions_file.stem, f\"mAP new classes at crop level: {new_mAP: .3f}\")\n",
    "\n",
    "base_mAP_event, base_APi_event = mean_average_precision_score(base_queries, results_event_df)\n",
    "new_mAP_event, new_APi_event = mean_average_precision_score(new_queries, results_event_df)\n",
    "print(\"*\", predictions_file.stem, f\"mAP base classes at event level: {base_mAP_event: .3f}\")\n",
    "print(\"*\", predictions_file.stem, f\"mAP new classes at event level: {new_mAP_event: .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be6c96-e751-4b4e-89c9-1413bef1c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(base_APi_event.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6feeaf-deba-40b6-ab00-cbb95f34225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(new_APi_event.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2effbe-6f39-48bf-853e-04a6645514eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Image Retrieval Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd0cf5-1ff3-4f90-a8d9-cab3fb112afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Be careful with the number of queries to plot\n",
    "for query in sorted(base_queries + new_queries):\n",
    "    print(query)\n",
    "\n",
    "    #We drop duplicates at the event level -- we are interested in retrieving separate events\n",
    "    top_10_annotations = results_df.sort_values(query, ascending=False).drop_duplicates(\"CaptureEventID\", keep=\"first\").head(10)\n",
    "    preds = top_10_annotations[query].values\n",
    "    \n",
    "    fig, axs = plt.subplots(2, \n",
    "                            5,\n",
    "                            figsize=(4*5, 4*2), \n",
    "                            gridspec_kw={\"wspace\":0.01, \"hspace\":0.01},\n",
    "                            squeeze=True)\n",
    "    axs = axs.flatten()\n",
    "    for i, row in enumerate(top_10_annotations.itertuples()):\n",
    "        crop_name = row.crop_path\n",
    "        img = viz.open_img(data_path / crop_name, max_dims=600)\n",
    "        if query in row.true:\n",
    "            img = viz.add_border_color(img, \"green\", width=8)\n",
    "        else:\n",
    "            img = viz.add_border_color(img, \"red\", width=8)\n",
    "        axs[i].set_axis_off()\n",
    "        axs[i].imshow(img, aspect=\"equal\") \n",
    "        axs[i].patch.set_linewidth('10')  \n",
    "        axs[i].text(20, 20, str(round(preds[i], 3)), backgroundcolor=\"w\")\n",
    "        axs[i].text(20, 370, Path(row.crop_path).stem, backgroundcolor=\"w\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
